// library for working with Mangolia token streams and syntax trees

{
	println: println
	slice: slice
	contains?: contains?
	map: map
	each: each
	filter: filter
	reduce: reduce
} := import('std')

{
	digit?: digit?
	word?: word?
	space?: space?
	contains?: strContains?
	trim: trim
} := import('str')

{
	format: format
} := import('fmt')

fn renderPos(pos) '[' + string(pos.0) + ':' + string(pos.1) + ']'

fn renderToken(token) if token.val {
	? -> format('{{ 0 }} {{ 1 }}', string(token.type), renderPos(token.pos))
	_ -> format('{{ 0 }}({{ 1 }}) {{ 2 }}', string(token.type), token.val, renderPos(token.pos))
}

// Tokenizer is a full-fidelity, lossless tokenizer for Oak. It produces a
// stream of valid Oak token types, plus any shebang, newlines, and comments it
// finds. To produce an AST, those non-standard non-AST tokens should be
// filtered out of the list first.
fn Tokenizer(source) {
	index := 0
	line := 1
	col := 0

	fn TokenAt(type, pos, val) {
		type: type
		val: val
		pos: pos
	}

	fn Token(type, val) TokenAt(type, [line, col], val)

	fn pos [line, col]
	fn eof? index = len(source)
	fn peek source.(index)
	fn peekAhead(n) if index + n > len(source) {
		true -> ' '
		_ -> source.(index + n)
	}
	fn next {
		char := source.(index)
		if index < len(source) {
			true -> index <- index + 1
		}
		if char {
			'\n' -> {
				line <- line + 1
				col <- 0
			}
			_ -> col <- col + 1
		}
		char
	}
	fn back {
		if index > 0 {
			true -> index <- index - 1
		}
		if source.(index) {
			// TODO: reset col correctly on backtrack
			'\n' -> line <- line - 1
			_ -> col <- col - 1
		}
	}

	fn readUntilChar(c) {
		fn sub(acc) if !eof?() & peek() != c {
			true -> sub(acc << next())
			_ -> acc
		}
		sub('')
	}
	fn readValidIdentifier {
		fn sub(acc) if eof?() {
			true -> acc
			_ -> {
				c := next()
				if word?(c) | c = '_' | c = '?' | c = '!' {
					true -> sub(acc << c)
					_ -> {
						back()
						acc
					}
				}
			}
		}
		sub('')
	}
	fn readValidNumeral {
		fn sub(acc) if eof?() {
			true -> acc
			_ -> {
				c := next()
				if digit?(c) | c = '.' {
					true -> sub(acc << c)
					_ -> {
						back()
						acc
					}
				}
			}
		}
		sub('')
	}
	fn nextToken if c := next() {
		',' -> Token(:comma)
		'.' -> if peek() = '.' & peekAhead(1) = '.' {
			true -> {
				pos := [line, col]
				next()
				next()
				TokenAt(:ellipsis, pos)
			}
			_ -> Token(:dot)
		}
		'(' -> Token(:leftParen)
		')' -> Token(:rightParen)
		'[' -> Token(:leftBracket)
		']' -> Token(:rightBracket)
		'{' -> Token(:leftBrace)
		'}' -> Token(:rightBrace)
		':' -> if peek() {
			'=' -> {
				pos := [line, col]
				next()
				TokenAt(:assign, pos)
			}
			_ -> Token(:colon)
		}
		'<' -> if peek() {
			'<' -> {
				pos := [line, col]
				next()
				TokenAt(:pushArrow, pos)
			}
			'-' -> {
				pos := [line, col]
				next()
				TokenAt(:nonlocalAssign, pos)
			}
			'=' -> {
				pos := [line, col]
				next()
				TokenAt(:leq, pos)
			}
			_ -> Token(:less)
		}
		'?' -> Token(:qmark)
		'!' -> if peek() {
			'=' -> {
				pos := [line, col]
				next()
				TokenAt(:neq, pos)
			}
			_ -> Token(:exclam)
		}
		'+' -> Token(:plus)
		'-' -> if peek() {
			'>' -> {
				pos := [line, col]
				next()
				TokenAt(:branchArrow, pos)
			}
			_ -> Token(:minus)
		}
		'*' -> Token(:times)
		'/' -> if peek() {
			'/' -> {
				// line comment
				pos := [line, col]
				next()
				commentString := readUntilChar('\n') |> trim()
				TokenAt(:comment, pos, commentString)
			}
			_ -> Token(:divide)
		}
		'%' -> Token(:modulus)
		'^' -> Token(:xor)
		'&' -> Token(:and)
		'|' -> if peek() {
			'>' -> {
				pos := [line, col]
				next()
				TokenAt(:pipeArrow, pos)
			}
			_ -> Token(:or)
		}
		'>' -> if peek() {
			'=' -> {
				pos := [line, col]
				next()
				TokenAt(:geq, pos)
			}
			_ -> Token(:greater)
		}
		'=' -> Token(:eq)
		'\'' -> {
			// TODO: support literal newlines, with extra tabs collapsed to newlines
			// TODO: support unicode escape sequences, like '\x10' = '\n' = char(10)
			fn sub(payload) if charInString := next() {
				? -> payload
				'\'' -> payload
				'\\' -> {
					if charInString := next() {
						'n' -> charInString := '\n'
						'r' -> charInString := '\r'
						'f' -> charInString := '\f'
						't' -> charInString := '\t'
					}
					sub(payload << charInString)
				}
				_ -> sub(payload << charInString)
			}
			pos := [line, col]
			stringContent := sub('')
			TokenAt(:stringLiteral, pos, stringContent)
		}
		_ -> {
			pos := [line, col]
			if digit?(c) {
				true -> {
					numberContent := c << readValidNumeral()
					TokenAt(:numberLiteral, pos, numberContent)
				}
				_ -> if payload := c << readValidIdentifier() {
					'_' -> TokenAt(:underscore, pos)
					'if' -> TokenAt(:ifKeyword, pos)
					'fn' -> TokenAt(:fnKeyword, pos)
					'with' -> TokenAt(:withKeyword, pos)
					'true' -> TokenAt(:trueLiteral, pos)
					'false' -> TokenAt(:falseLiteral, pos)
					_ -> TokenAt(:identifier, pos, payload)
				}
			}
		}
	}
	fn tokenize {
		tokens := []

		if peek() = '#' & peekAhead(1) = '!' {
			true -> {
				readUntilChar('\n')
				if !eof?() {
					true -> next()
				}
			}
		}

		// snip whitespace before
		fn eatSpace if space?(sp := peek()){
			true -> {
				if sp {
					'\n' -> tokens << Token(:newline)
				}
				next()
				eatSpace()
			}
		}
		eatSpace()

		lastTok := Token(:comma)
		fn sub {
			nextTok := nextToken()

			if !([:leftParen, :leftBracket, :leftBrace, :comma] |> contains?(lastTok.type)) &
				[:rightParen, :rightBracket, :rightBrace] |> contains?(nextTok.type) {
				true -> tokens << Token(:comma)
			}

			tokens << nextTok
			if nextTok.type {
				:comment -> nextTok := lastTok
			}

			// snip whitespace after
			fn eatSpaceAutoInsertComma if space?(peek()){
				true -> {
					if peek() {
						'\n' -> {
							if [
								:comma, :leftParen, :leftBracket, :leftBrace,
								:plus, :minus, :times, :divide, :modulus, :xor,
								:and, :or, :exclam, :greater, :less, :eq, :geq,
								:leq, :assign, :nonlocalAssign, :dot, :colon,
								:fnKeyword, :ifKeyword, :withKeyword,
								:pipeArrow, :branchArrow,
							] |> contains?(nextTok.type) {
								true -> ? // do nothing
								_ -> {
									nextTok <- Token(:comma)
									tokens << nextTok
								}
							}
							tokens << Token(:newline)
						}
					}
					next()
					eatSpaceAutoInsertComma()
				}
			}
			eatSpaceAutoInsertComma()

			if nextTok.type {
				:comment -> ?
				_ -> lastTok <- nextTok
			}

			if eof?() {
				false -> sub()
			}
		}

		// do not start tokenizing into empty file
		if eof?() {
			false -> sub()
		}

		if lastTok.type {
			:comma -> ?
			_ -> tokens << Token(:comma)
		}

		tokens
	}

	{
		tokenize: tokenize
	}
}

// Parser takes a raw token stream, potentially including newlines and
// comments, and generates a list of clean Oak AST nodes.
fn Parser(tokens) {
	index := 0
	minBinaryPrec := [0]

	fn error(msg, pos) {
		type: :error
		error: msg
		pos: pos
	}

	fn lastMinPrec minBinaryPrec.(len(minBinaryPrec) - 1)
	fn pushMinPrec(prec) minBinaryPrec << prec
	fn popMinPrec minBinaryPrec <- slice(minBinaryPrec, 1, len(minBinaryPrec) - 1)
	fn eof? index = len(tokens)
	fn peek tokens.(index)
	fn peekAhead(n) if index + n > len(tokens) {
		true -> {type: :comma}
		_ -> tokens.(index + n)
	}
	fn next {
		tok := tokens.(index)
		if index < len(tokens) {
			true -> index <- index + 1
		}
		tok
	}
	fn back if index > 0 {
		true -> index <- index - 1
	}
	fn expect(type) if eof?() {
		true -> error(format('Unexpected end of input, expected {{0}}', type))
		_ -> {
			next := next()
			if next.type {
				type -> next
				_ -> error(format('Unexpected token {{0}}, expected {{1}}', next, type), next.pos)
			}
		}
	}
	fn readUntilTokenType(type) {
		tokens := []
		fn sub if !eof() & peek().type != type {
			true -> {
				tokens << next()
				sub()
			}
			_ -> tokens
		}
		sub()
	}

	fn notError(x, withNotErr) if x.type {
		:error -> x
		_ -> withNotErr(x)
	}

	// parseUnit is responsible for parsing the smallest complete syntactic
	// "units" of Oak's syntax, like literals including function literals,
	// grouped expressions in blocks, and if/with expressions.
	fn parseUnit {
		tok := next()
		if tok.type {
			:qmark -> { type: :null, tok: tok }
			:stringLiteral -> {
				type: :string
				tok: tok
				payload: tok.payload
			}
			:numberLiteral -> if tok.payload |> strContains?('.') {
				true -> if parsed := float(tok.payload) {
					? -> error(format('Could not parse floating point number {{0}}', tok.payload), tok.pos)
					_ -> {
						type: :number
						tok: tok
						int?: false
						payload: parsed
					}
				}
				_ -> if parsed := int(tok.payload) {
					? -> error(format('Could not parse integer number {{0}}', tok.payload), tok.pos)
					_ -> {
						type: :number
						tok: tok
						int?: true
						payload: parsed
					}
				}
			}
			:trueLiteral -> {
				type: :boolean
				tok: tok
				payload: true
			}
			:falseLiteral -> {
				type: :boolean
				tok: tok
				payload: false
			}
			:colon -> if peek().type {
				:identifier -> {
					type: :atom
					tok: tok
					payload: next().payload
				}
				_ -> error(format('Expected identifier after ":", got {{0}}', peek()), peek().pos)
			}
			:leftBracket -> {
				pushMinPrec(0)

				itemNodes := []
				fn sub if !eof?() & peek().kind != :rightBracket {
					true -> with notError(node := parseNode) fn {
						with notError(err := expect(:comma)) fn {
							itemNodes << node
							sub()
						}
					}
				}
				with notError(sub()) {
					with notError(err := expect(:rightBracket)) fn {
						popMinPrec()

						{
							type: :list
							tok: tok
							elems: itemNodes
						}
					}
				}
			}
			:leftBrace -> {
				pushMinPrec(0)

				// empty {} is always considered an object -- an empty block is illegal
				if peek().type {
					:rightBrace -> {
						next() // eat the rightBrace
						popMinPrec()

						{
							type: :object
							tok: tok
							entries: []
						}
					}
					_ -> with notError(firstExpr := parseNode()) fn if eof?() {
						true -> error('Unexpected end of input inside block or object', tok.pos)
						_ -> if peek().type {
							:colon -> {
								// it's an object
								next() // eat the colon
								with notError(valExpr := parseNode()) fn {
									with notError(expect(:comma)) fn {
										entries := [{ key: firstExpr, val: valExpr }]

										fn sub if !eof?() & peek().type != :rightBrace {
											true -> with notError(key := parseNode()) fn {
												with notError(expect(:colon)) fn {
													with notError(val := parseNode()) fn {
														with notError(expect(:comma)) fn {
															entries << { key: key, val: val }
															sub()
														}
													}
												}
											}
										}
										with notError(sub()) fn {
											with notError(expect(:rightBrace)) fn {
												popMinPrec()

												{
													type: :object
													tok: tok
													entries: entries
												}
											}
										}
									}
								}
							}
							_ -> with notError(expect(:comma)) fn {
								// it's a block
								exprs := []

								fn sub if !eof?() & peek().type != :rightBrace {
									true -> with notError(expr := parseNode()) fn {
										with notError(expect(:comma)) fn {
											exprs << expr
											sub()
										}
									}
								}
								with notError(sub()) fn {
									with notError(expect(:rightBrace)) fn {
										popMinPrec()

										{
											type: :block
											tok: tok
											exprs: exprs
										}
									}
								}
							}
						}
					}
				}
			}
			:fnKeyword -> {
				pushMinPrec(0)

				name := if peek().type {
					// optional named fn
					:identifier -> next().payload
					_ -> ''
				}

				args := []
				restArg := ''

				fn parseBody with notError(body := parseNode()) fn {
					// Exception to the "{} is empty object" rule is that `fn
					// {}` parses as a function with an empty block as a body.
					if body.type = :object & len(body.entries) = 0 {
						true -> body <- {
							type: :block
							tok: body.tok
							exprs: []
						}
					}

					{
						type: :function
						tok: tok
						args: args
						restArg: restArg
						body: body
					}
				}

				if peek().type {
					:leftParen -> {
						// optional argument list
						next() // eat the leftParen

						fn sub if !eof?() & peek().type != :rightParen {
							true -> {
								arg := expect(:identifier)
								if arg.type {
									:error -> {
										back() // try again

										with notError(expect(:underscore)) fn {
											args << ''
											with notError(:comma) fn {
												sub()
											}
										}
									}
									:ellipsis -> {
										restArg <- arg.payload
										next() // eat the ellipsis
										with notError(expect(:comma)) fn {
											sub()
										}
									}
									_ -> {
										args << arg.payload
										with notError(expect(:comma)) fn {
											sub()
										}
									}
								}
							}
						}
						with notError(expect(:rightParen)) fn {
							parseBody()
						}
					}
					_ -> parseBody()
				}
			}
			:underscore -> {
				type: :empty
				tok: tok
			}
			:identifier -> {
				type: :identifier
				tok: tok
				payload: tok.payload
			}
			:minus, :exclam -> with notError(right := parseSubNode()) fn {
				type: :unary
				tok: :tok
				op: tok.type
				right: right
			}
			:ifKeyword -> {
				pushMinPrec(0)
				// TODO: if keyword
			}
			:withKeyword -> {
				pushMinPrec(0)
				// TODO: with keyword
			}
			:leftParen -> {
				pushMinPrec(0)
				// TODO: left paren
			}
			_ -> error(format('Unexpected token {{0}} at start of unit', tok), tok.pos)
		}
	}

	fn infixOpPrecedence(op) if op {
		:plus, :minus -> 40
		:times, :divide -> 50
		:modulus -> 80
		:eq, :greater, :less, :geq, :leq, :neq -> 30
		:and -> 20
		:xor -> 15
		:or -> 10
		// assignment-like semantics
		:pushArrow -> 1
		_ -> -1
	}

	// parseSubNode is responsible for parsing independent "terms" in the Oak
	// syntax, like terms in unary and binary expressions and in pipelines. It
	// is in between parseUnit and parseNode.
	fn parseSubNode {
		// TODO: parseSubNode
	}

	// parseNode returns the next top-level astNode from the parser
	fn parseNode {
		// TODO: parseNode
	}

	// for parsing purposes, we must ignore non-semantic tokens
	tokens := tokens |> filter(fn(tok) if tok.type {
		:newline -> false
		:comment -> false
		_ -> true
	})

	// parse
	nodes := []
	fn sub if !eof?() {
		true -> with notError(node := parseNode()) fn {
			with notError(expect(:comma)) fn {
				nodes << node
			}
		}
	}
	with notError(sub()) fn {
		nodes
	}
}

