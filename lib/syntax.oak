// libsyntax implements a tokenier and parser for the Oak language

{
	println: println
	default: default
	range: range
	slice: slice
	append: append
	contains?: contains?
	map: map
	each: each
	filter: filter
	reduce: reduce
} := import('std')
{
	digit?: digit?
	word?: word?
	space?: space?
	contains?: strContains?
	join: join
	replace: replace
	trimStart: trimStart
	trim: trim
} := import('str')
{
	min: min
} := import('math')
{
	format: format
	printf: printf
} := import('fmt')

fn renderPos(pos) '[' + string(pos.0) + ':' + string(pos.1) + ']'

fn renderToken(token) if token.val {
	? -> format('{{ 0 }} {{ 1 }}', string(token.type), renderPos(token.pos))
	_ -> format('{{ 0 }}({{ 1 }}) {{ 2 }}', string(token.type), token.val, renderPos(token.pos))
}

// Tokenizer is a full-fidelity, lossless tokenizer for Oak. It produces a
// stream of valid Oak token types, plus any shebang, newlines, and comments it
// finds. To produce an AST, those non-standard non-AST tokens should be
// filtered out of the list first.
//
// Methods:
//
// fn tokenize()    returns a list of tokens
fn Tokenizer(source) {
	index := 0
	line := 1
	col := 0

	fn TokenAt(type, pos, val) {
		type: type
		val: val
		pos: pos
	}

	fn Token(type, val) TokenAt(type, [line, col], val)

	fn pos [line, col]
	fn eof? index = len(source)
	fn peek source.(index)
	fn peekAhead(n) if index + n > len(source) {
		true -> ' '
		_ -> source.(index + n)
	}
	fn next {
		char := source.(index)
		if index < len(source) {
			true -> index <- index + 1
		}
		if char {
			'\n' -> {
				line <- line + 1
				col <- 0
			}
			_ -> col <- col + 1
		}
		char
	}
	fn back {
		if index > 0 {
			true -> index <- index - 1
		}
		if source.(index) {
			// TODO: reset col correctly on backtrack
			'\n' -> line <- line - 1
			_ -> col <- col - 1
		}
	}

	fn readUntilChar(c) {
		fn sub(acc) if !eof?() & peek() != c {
			true -> sub(acc << next())
			_ -> acc
		}
		sub('')
	}
	fn readValidIdentifier {
		fn sub(acc) if eof?() {
			true -> acc
			_ -> {
				c := next()
				if word?(c) | c = '_' | c = '?' | c = '!' {
					true -> sub(acc << c)
					_ -> {
						back()
						acc
					}
				}
			}
		}
		sub('')
	}
	fn readValidNumeral {
		fn sub(acc) if eof?() {
			true -> acc
			_ -> {
				c := next()
				if digit?(c) | c = '.' {
					true -> sub(acc << c)
					_ -> {
						back()
						acc
					}
				}
			}
		}
		sub('')
	}
	fn nextToken if c := next() {
		',' -> Token(:comma)
		'.' -> if peek() = '.' & peekAhead(1) = '.' {
			true -> {
				pos := [line, col]
				next()
				next()
				TokenAt(:ellipsis, pos)
			}
			_ -> Token(:dot)
		}
		'(' -> Token(:leftParen)
		')' -> Token(:rightParen)
		'[' -> Token(:leftBracket)
		']' -> Token(:rightBracket)
		'{' -> Token(:leftBrace)
		'}' -> Token(:rightBrace)
		':' -> if peek() {
			'=' -> {
				pos := [line, col]
				next()
				TokenAt(:assign, pos)
			}
			_ -> Token(:colon)
		}
		'<' -> if peek() {
			'<' -> {
				pos := [line, col]
				next()
				TokenAt(:pushArrow, pos)
			}
			'-' -> {
				pos := [line, col]
				next()
				TokenAt(:nonlocalAssign, pos)
			}
			'=' -> {
				pos := [line, col]
				next()
				TokenAt(:leq, pos)
			}
			_ -> Token(:less)
		}
		'?' -> Token(:qmark)
		'!' -> if peek() {
			'=' -> {
				pos := [line, col]
				next()
				TokenAt(:neq, pos)
			}
			_ -> Token(:exclam)
		}
		'+' -> Token(:plus)
		'-' -> if peek() {
			'>' -> {
				pos := [line, col]
				next()
				TokenAt(:branchArrow, pos)
			}
			_ -> Token(:minus)
		}
		'*' -> Token(:times)
		'/' -> if peek() {
			'/' -> {
				// line comment
				pos := [line, col]
				next()
				commentString := readUntilChar('\n') |> trim()
				TokenAt(:comment, pos, commentString)
			}
			_ -> Token(:divide)
		}
		'%' -> Token(:modulus)
		'^' -> Token(:xor)
		'&' -> Token(:and)
		'|' -> if peek() {
			'>' -> {
				pos := [line, col]
				next()
				TokenAt(:pipeArrow, pos)
			}
			_ -> Token(:or)
		}
		'>' -> if peek() {
			'=' -> {
				pos := [line, col]
				next()
				TokenAt(:geq, pos)
			}
			_ -> Token(:greater)
		}
		'=' -> Token(:eq)
		'\'' -> {
			// TODO: support literal newlines, with extra tabs collapsed to newlines
			// TODO: support unicode escape sequences, like '\x10' = '\n' = char(10)
			fn sub(payload) if charInString := next() {
				? -> payload
				'\'' -> payload
				'\\' -> {
					if charInString := next() {
						'n' -> charInString := '\n'
						'r' -> charInString := '\r'
						'f' -> charInString := '\f'
						't' -> charInString := '\t'
					}
					sub(payload << charInString)
				}
				_ -> sub(payload << charInString)
			}
			pos := [line, col]
			stringContent := sub('')
			TokenAt(:stringLiteral, pos, stringContent)
		}
		_ -> {
			pos := [line, col]
			if digit?(c) {
				true -> {
					numberContent := c << readValidNumeral()
					TokenAt(:numberLiteral, pos, numberContent)
				}
				_ -> if payload := c << readValidIdentifier() {
					'_' -> TokenAt(:underscore, pos)
					'if' -> TokenAt(:ifKeyword, pos)
					'fn' -> TokenAt(:fnKeyword, pos)
					'with' -> TokenAt(:withKeyword, pos)
					'true' -> TokenAt(:trueLiteral, pos)
					'false' -> TokenAt(:falseLiteral, pos)
					_ -> TokenAt(:identifier, pos, payload)
				}
			}
		}
	}
	fn tokenize {
		tokens := []

		if peek() = '#' & peekAhead(1) = '!' {
			true -> {
				readUntilChar('\n')
				if !eof?() {
					true -> next()
				}
			}
		}

		// snip whitespace before
		fn eatSpace if space?(sp := peek()){
			true -> {
				if sp {
					'\n' -> tokens << Token(:newline)
				}
				next()
				eatSpace()
			}
		}
		eatSpace()

		lastTok := Token(:comma)
		fn sub {
			nextTok := nextToken()

			if !([:leftParen, :leftBracket, :leftBrace, :comma] |> contains?(lastTok.type)) &
				[:rightParen, :rightBracket, :rightBrace] |> contains?(nextTok.type) {
				true -> tokens << Token(:comma)
			}

			tokens << nextTok
			if nextTok.type {
				:comment -> nextTok := lastTok
			}

			// snip whitespace after
			fn eatSpaceAutoInsertComma if space?(peek()){
				true -> {
					if peek() {
						'\n' -> {
							if [
								:comma, :leftParen, :leftBracket, :leftBrace,
								:plus, :minus, :times, :divide, :modulus, :xor,
								:and, :or, :exclam, :greater, :less, :eq, :geq,
								:leq, :assign, :nonlocalAssign, :dot, :colon,
								:fnKeyword, :ifKeyword, :withKeyword,
								:pipeArrow, :branchArrow,
							] |> contains?(nextTok.type) {
								true -> ? // do nothing
								_ -> {
									nextTok <- Token(:comma)
									tokens << nextTok
								}
							}
							tokens << Token(:newline)
						}
					}
					next()
					eatSpaceAutoInsertComma()
				}
			}
			eatSpaceAutoInsertComma()

			if nextTok.type {
				:comment -> ?
				_ -> lastTok <- nextTok
			}

			if eof?() {
				false -> sub()
			}
		}

		// do not start tokenizing into empty file
		if eof?() {
			false -> sub()
		}

		if lastTok.type {
			:comma -> ?
			_ -> tokens << Token(:comma)
		}

		tokens
	}

	{
		tokenize: tokenize
	}
}

// tokenize takes Oak source text and returns a list of tokens
fn tokenize(text) Tokenizer(text).tokenize()

// Parser takes a raw token stream, potentially including newlines and
// comments, and generates a list of clean Oak AST nodes.
//
// Methods:
//
// fn parse()   returns a list of AST nodes
fn Parser(tokens) {
	index := 0
	minBinaryPrec := [0]

	// for parsing purposes, we must ignore non-semantic tokens
	tokens := tokens |> filter(fn(tok) if tok.type {
		:newline, :comment -> false
		_ -> true
	})

	fn error(msg, pos) {
		type: :error
		error: msg
		pos: pos
	}

	fn lastMinPrec minBinaryPrec.(len(minBinaryPrec) - 1)
	fn pushMinPrec(prec) minBinaryPrec << prec
	fn popMinPrec minBinaryPrec <- slice(minBinaryPrec, 0, len(minBinaryPrec) - 1)
	fn eof? index = len(tokens)
	fn peek tokens.(index)
	fn peekAhead(n) if index + n > len(tokens) {
		true -> { type: :comma }
		_ -> tokens.(index + n)
	}
	fn next {
		tok := tokens.(index)
		if index < len(tokens) {
			true -> index <- index + 1
		}
		tok
	}
	fn back if index > 0 {
		true -> index <- index - 1
	}
	fn expect(type) if eof?() {
		true -> error(format('Unexpected end of input, expected {{0}}', type))
		_ -> {
			next := next()
			if next.type {
				type -> next
				_ -> error(format('Unexpected token {{0}}, expected {{1}}', next, type), next.pos)
			}
		}
	}
	fn readUntilTokenType(type) {
		tokens := []
		fn sub if !eof() & peek().type != type {
			true -> {
				tokens << next()
				sub()
			}
			_ -> tokens
		}
		sub()
	}

	fn notError(x, withNotErr) if x {
		{ type: :error, error: _, pos: _ } -> x
		_ -> withNotErr(x)
	}

	fn parseAssignment(left) if peek().type {
		:assign, :nonlocalAssign -> {
			nxt := next()
			node := {
				type: :assignment
				tok: nxt
				local?: nxt.type = :assign
				left: left
			}

			with notError(right := parseNode()) fn {
				node.right := right
				node
			}
		}
		_ -> left
	}

	// parseUnit is responsible for parsing the smallest complete syntactic
	// "units" of Oak's syntax, like literals including function literals,
	// grouped expressions in blocks, and if/with expressions.
	fn parseUnit {
		tok := next()
		if tok.type {
			:qmark -> { type: :null, tok: tok }
			:stringLiteral -> {
				type: :string
				tok: tok
				val: tok.val
			}
			:numberLiteral -> if tok.val |> strContains?('.') {
				true -> if parsed := float(tok.val) {
					? -> error(format('Could not parse floating point number {{0}}', tok.val), tok.pos)
					_ -> {
						type: :number
						tok: tok
						int?: false
						val: parsed
					}
				}
				_ -> if parsed := int(tok.val) {
					? -> error(format('Could not parse integer number {{0}}', tok.val), tok.pos)
					_ -> {
						type: :number
						tok: tok
						int?: true
						val: parsed
					}
				}
			}
			:trueLiteral -> {
				type: :boolean
				tok: tok
				val: true
			}
			:falseLiteral -> {
				type: :boolean
				tok: tok
				val: false
			}
			:colon -> if peek().type {
				:identifier -> {
					type: :atom
					tok: tok
					val: next().val
				}
				_ -> error(format('Expected identifier after ":", got {{0}}', peek()), peek().pos)
			}
			:leftBracket -> {
				pushMinPrec(0)

				itemNodes := []
				fn sub if !eof?() & peek().type != :rightBracket {
					true -> with notError(node := parseNode()) fn {
						with notError(err := expect(:comma)) fn {
							itemNodes << node
							sub()
						}
					}
				}
				with notError(sub()) fn {
					with notError(err := expect(:rightBracket)) fn {
						popMinPrec()

						{
							type: :list
							tok: tok
							elems: itemNodes
						}
					}
				}
			}
			:leftBrace -> {
				pushMinPrec(0)

				// empty {} is always considered an object -- an empty block is illegal
				if peek().type {
					:rightBrace -> {
						next() // eat the rightBrace
						popMinPrec()

						{
							type: :object
							tok: tok
							entries: []
						}
					}
					_ -> with notError(firstExpr := parseNode()) fn if eof?() {
						true -> error('Unexpected end of input inside block or object', tok.pos)
						_ -> if peek().type {
							:colon -> {
								// it's an object
								next() // eat the colon
								with notError(valExpr := parseNode()) fn {
									with notError(expect(:comma)) fn {
										entries := [{ key: firstExpr, val: valExpr }]

										fn sub if eof?() {
											false -> if peek().type {
												:rightBrace -> ?
												_ -> with notError(key := parseNode()) fn {
													with notError(expect(:colon)) fn {
														with notError(val := parseNode()) fn {
															with notError(expect(:comma)) fn {
																entries << { key: key, val: val }
																sub()
															}
														}
													}
												}
											}
										}
										with notError(sub()) fn {
											with notError(expect(:rightBrace)) fn {
												popMinPrec()

												{
													type: :object
													tok: tok
													entries: entries
												}
											}
										}
									}
								}
							}
							_ -> with notError(expect(:comma)) fn {
								// it's a block
								exprs := [firstExpr]

								fn sub if !eof?() & peek().type != :rightBrace {
									true -> with notError(expr := parseNode()) fn {
										with notError(expect(:comma)) fn {
											exprs << expr
											sub()
										}
									}
								}
								with notError(sub()) fn {
									with notError(expect(:rightBrace)) fn {
										popMinPrec()

										{
											type: :block
											tok: tok
											exprs: exprs
										}
									}
								}
							}
						}
					}
				}
			}
			:fnKeyword -> {
				pushMinPrec(0)

				name := if peek().type {
					// optional named fn
					:identifier -> next().val
					_ -> ''
				}

				args := []
				restArg := ''

				fn parseBody with notError(body := parseNode()) fn {
					// Exception to the "{} is empty object" rule is that `fn
					// {}` parses as a function with an empty block as a body.
					if body {
						{ type: :object, tok: _, entries: [] } -> body <- {
							type: :block
							tok: body.tok
							exprs: []
						}
					}
					popMinPrec()

					{
						type: :function
						name: name
						tok: tok
						args: args
						restArg: restArg
						body: body
					}
				}

				if peek().type {
					:leftParen -> {
						// optional argument list
						next() // eat the leftParen

						fn sub if eof?() {
							false -> if peek().type {
								:rightParen -> ?
								_ -> {
									arg := expect(:identifier)
									if arg.type {
										:error -> {
											back() // try again

											with notError(expect(:underscore)) fn {
												args << ''
												with notError(:comma) fn {
													sub()
												}
											}
										}
										_ -> if peek().type {
											:ellipsis -> {
												restArg <- arg.val
												next() // eat the ellipsis
												with notError(expect(:comma)) fn {
													sub()
												}
											}
											_ -> {
												args << arg.val
												with notError(expect(:comma)) fn {
													sub()
												}
											}
										}
									}
								}
							}
						}
						with notError(sub()) fn {
							with notError(expect(:rightParen)) fn {
								parseBody()
							}
						}
					}
					_ -> parseBody()
				}
			}
			:underscore -> {
				type: :empty
				tok: tok
			}
			:identifier -> {
				type: :identifier
				tok: tok
				val: tok.val
			}
			:minus, :exclam -> with notError(right := parseSubNode()) fn {
				type: :unary
				tok: tok
				op: tok.type
				right: right
			}
			:ifKeyword -> {
				// We want to support multi-target branches, but don't want to
				// incur the performance overhead in the interpreter/evaluator
				// of keeping every single target as a Go slice, when the vast
				// majority of targets will be single-value, which requires
				// just a pointer to an astNode.
				//
				// So instead of doing that, we penalize the multi-value case
				// by essentially considering it syntax sugar and splitting
				// such branches into multiple AST branches, each with one
				// target value.
				pushMinPrec(0)

				// if no explicit condition is provided (i.e. if the keyword is
				// followed by a { ... }), we assume the condition is "true" to
				// allow for the useful `if { case, case ... }` pattern.
				condNode := if peek().type {
					:leftBrace -> {
						type: :boolean
						val: true
						tok: tok
					}
					_ -> parseNode()
				}

				with notError(condNode) fn {
					with notError(expect(:leftBrace)) fn {
						fn subBranch(branches) if eof?() {
							false -> if peek().type {
								:rightBrace -> branches
								_ -> {
									fn subTarget(targets) if eof?() {
										true -> targets
										_ -> with notError(target := parseNode()) fn if peek().type {
											:branchArrow -> targets << target
											_ -> with notError(expect(:comma)) fn {
												subTarget(targets << target)
											}
										}
									}
									with notError(targets := subTarget([])) fn {
										with notError(expect(:branchArrow)) fn {
											with notError(body := parseNode()) fn {
												with notError(expect(:comma)) fn {
													 subBranch(branches |> append(targets |> with map() fn(target) {
														type: :ifBranch
														target: target
														body: body
													}))
												}
											}
										}
									}
								}
							}
							_ -> branches
						}
						with notError(branches := subBranch([])) fn {
							with notError(expect(:rightBrace)) fn {
								popMinPrec()

								{
									type: :ifExpr
									tok: tok
									cond: condNode
									branches: branches
								}
							}
						}
					}
				}
			}
			:withKeyword -> {
				pushMinPrec(0)
				with notError(base := parseNode()) fn if base.type {
					:fnCall -> with notError(lastArg := parseNode()) fn {
						popMinPrec()
						base.args << lastArg
						base
					}
					_ -> error(format('with keyword should be followed by a fn call, found {{0}}', base), tok.pos)
				}
			}
			:leftParen -> {
				pushMinPrec(0)

				fn subExpr(exprs) if !eof?() & peek().type != :rightParen {
					true -> with notError(expr := parseNode()) fn {
						with notError(expect(:comma)) fn {
							subExpr(exprs << expr)
						}
					}
					_ -> exprs
				}
				with notError(exprs := subExpr([])) fn {
					with notError(expect(:rightParen)) fn {
						popMinPrec()
						{
							type: :block
							tok: tok
							exprs: exprs
						}
					}
				}
			}
			_ -> error(format('Unexpected token {{0}} at start of unit', tok), tok.pos)
		}
	}

	fn infixOpPrecedence(op) if op {
		:plus, :minus -> 40
		:times, :divide -> 50
		:modulus -> 80
		:eq, :greater, :less, :geq, :leq, :neq -> 30
		:and -> 20
		:xor -> 15
		:or -> 10
		// assignment-like semantics
		:pushArrow -> 1
		_ -> -1
	}

	// parseSubNode is responsible for parsing independent "terms" in the Oak
	// syntax, like terms in unary and binary expressions and in pipelines. It
	// is in between parseUnit and parseNode.
	fn parseSubNode {
		pushMinPrec(0)
		with notError(node := parseUnit()) fn {
			fn sub if eof?() {
				false -> if peek().type {
					:dot -> {
						nxt := next() // eat the dot
						with notError(right := parseUnit()) fn {
							node <- {
								type: :propertyAccess
								tok: nxt
								left: node
								right: right
							}
							sub()
						}
					}
					:leftParen -> {
						nxt := next() // eat the leftParen

						args := []
						restArg := ?
						fn subArg if eof?() {
							false -> if peek().type {
								:rightParen -> with notError(expect(:rightParen)) fn {
									?
								}
								_ -> with notError(arg := parseNode()) fn if peek().type {
									:ellipsis -> {
										next() // eat the ellipsis
										with notError(expect(:comma)) fn {
											restArg <- arg
											subArg()
										}
									}
									:comma -> {
										next() // eat the comma
										args << arg
										subArg()
									}
									_ -> error(format('Expected comma after arg in argument list, got {{0}}', peek().type), peek().pos)
								}
							}
						}
						with notError(subArg()) fn {
							node <- {
								type: :fnCall
								function: node
								args: args
								restArg: restArg
								tok: nxt
							}
							sub()
						}
					}
				}
			}
			with notError(sub()) fn {
				popMinPrec()
				node
			}
		}
	}

	// parseNode returns the next top-level astNode from the parser
	fn parseNode with notError(node := parseSubNode()) fn {
		fn sub if eof?() {
			false -> if peek().type {
				:comma -> ?
				// whatever follows an assignment expr cannot bind to the
				// assignment expression itself by syntax rule, so we simply
				// return
				:assign, :nonlocalAssign -> node <- parseAssignment(node)
				// this case implements a mini Pratt parser threaded through
				// the larger Oak syntax parser, using the parser struct itself
				// to keep track of the power / precedence stack since other
				// forms may be parsed in between, as in 1 + f(g(x := y)) + 2
				:plus, :minus, :times, :divide, :modulus, :xor, :and, :or,
				:pushArrow, :greater, :less, :eq, :geq, :leq, :neq -> {
					minPrec := lastMinPrec()
					fn subBinary if eof?() {
						true -> error('Incomplete binary expression', peek().pos)
						_ -> {
							peeked := peek()
							op := peeked.type
							prec := infixOpPrecedence(op)
							if prec <= minPrec {
								false -> {
									next() // eat the operator

									if eof?() {
										true -> error(format('Incomplete binary expression with {{0}}', {type: op}), peek().pos)
										_ -> {
											pushMinPrec(prec)
											with notError(right := parseNode()) fn {
												popMinPrec()

												node <- {
													type: :binary
													tok: peeked
													op: op
													left: node
													right: right
												}
												subBinary()
											}
										}
									}
								}
							}
						}
					}
					with notError(subBinary()) fn {
						// whatever follows a binary expr cannot bind to the
						// binary expression by syntax rule, so we simply
						// return
						node
					}
				}
				:pipeArrow -> {
					pipe := next() // eat the pipe
					with notError(pipeRight := parseSubNode()) fn if pipeRight.type {
						:fnCall -> {
							pipeRight.args := append([node], pipeRight.args)
							node <- pipeRight
							sub()
						}
						_ -> error(format('Expected function call after |>, got {{0}}', pipeRight), pipe.pos)
					}
				}
			}
		}
		// the trailing comma is handled as necessary in callers of parseNode
		with notError(sub()) fn {
			node
		}
	}

	{
		parse: fn {
			// parse
			nodes := []
			fn sub if eof?() {
				false -> with notError(node := parseNode()) fn {
					with notError(expect(:comma)) fn {
						nodes << node
						sub()
					}
				}
			}
			with notError(sub()) fn {
				nodes
			}
		}
	}
}

// parse takes Oak source text and returns a list of AST nodes
fn parse(text) {
	tokens := tokenize(text)
	Parser(tokens).parse()
}

// Printer takes a list of Oak tokens and pretty-prints the source code into a
// string. As a rule, all newlines are preserved, including those in comments.
//
// Methods:
//
// fn print()   returns a pretty-printed string
fn Printer(tokens) {
	// create a string with N tabs in it
	fn tabs(n) if {
		n > 0 -> tabs(n - 1) << '\t'
		_ -> ''
	}

	// a single tone -> its printed value
	fn render(token) if token.type {
		:comment -> '// ' << token.val
		:comma -> ','
		:dot -> '.'
		:leftParen -> '('
		:rightParen -> ')'
		:leftBracket -> '['
		:rightBracket -> ']'
		:leftBrace -> '{'
		:rightBrace -> '}'
		:assign -> ':='
		:nonlocalAssign -> '<-'
		:pipeArrow -> '|>'
		:branchArrow -> '->'
		:pushArrow -> '<<'
		:colon -> ':'
		:ellipsis -> '...'
		:qmark -> '?'
		:exclam -> '!'
		:plus -> '+'
		:minus -> '-'
		:times -> '*'
		:divide -> '/'
		:modulus -> '%'
		:xor -> '^'
		:and -> '&'
		:or -> '|'
		:greater -> '>'
		:less -> '<'
		:eq -> '='
		:geq -> '>='
		:leq -> '<='
		:neq -> '!='
		:ifKeyword -> 'if'
		:fnKeyword -> 'fn'
		:withKeyword -> 'with'
		:underscore -> '_'
		:identifier -> token.val
		:trueLiteral -> 'true'
		:falseLiteral -> 'false'
		:stringLiteral -> '\'' << token.val |>
			replace('\\', '\\\\') |>
			replace('\'', '\\\'') |>
			replace('\n', '\\\\n') |>
			replace('\t', '\\\\t') << '\''
		:numberLiteral -> token.val
		_ -> {
			printf('Unknown token {{0}}', token)
			string(token)
		}
	}

	// does a token require a space to follow it in well-formatted code?
	fn connectingToken?(tokenType) if tokenType {
		:assign,
		:nonlocalAssign,
		:pipeArrow,
		:branchArrow,
		:pushArrow,
		:colon,
		:plus, :minus, :times, :divide, :modulus,
		:xor, :and, :or,
		:greater, :less, :eq, :geq, :leq, :neq -> true
		_ -> false
	}

	// shorthand for the last item of a list
	fn last(list) list.(len(list) - 1)

	fn print {
		// we keep track of lines of code and their corresponding
		// indent levels separately and merge them at the end.
		//
		// this turns out to be simpler than trying to adjust
		// indentations while also adding on lines of code.
		lines := ['']
		indents := []

		// algorithm state
		curr := 0
		// TODO: explain why this exists?
		currs := [0]
		// indicates whether the following line should have a hanging indent
		hanging? := false

		// shorthand for adding tokens and spaces to algorithm state
		fn add(s, tabs) {
			if last(lines) |> trim('\t') {
				'' -> last(lines) << trimStart(s, ' ')
				_ -> last(lines) << s
			}
			curr <- curr + tabs
			currs << curr
		}

		fn purelyDescendingPrefix(list) if len(list) {
			0 -> list
			_ -> {
				fn sub(i) if {
					i = len(list) -> list |> slice(0, i)
					list.(i - 1) > list.(i) -> sub(i + 1)
					_ -> list |> slice(0, i)
				}
				sub(1)
			}
		}

		fn indentLine(lastType) {
			indent := min(currs...)

			// account for hanging indent
			if {
				// TODO: explain what this line checks for / does!
				{
					prefix := purelyDescendingPrefix(currs)
					len(prefix) = 1 & default(prefix.0, 0) > indent
				}
				hanging? -> indent <- indent + 1
			}

			// set the hanging indent flag for this current line, so we can
			// indent accordingly at start of next line
			hanging? <- if {
				connectingToken?(lastType)
				lastType = :dot -> true
				_ -> false
			}

			indent
		}

		// in this loop, we ask whether a space should come before each token.
		// as a result: a token is only responsible for adding a space before
		// it, not after it
		tokens |> with each() fn(token, i) {
			llastType := default(tokens.(i - 2), { type: :newline }).type
			lastType := default(tokens.(i - 1), { type: :newline }).type
			nextType := default(tokens.(i + 1), { type: :newline }).type
			if [lastType, token.type, nextType] {
				// this match clause is responsible for computing the correct
				// level of indentation for the line that comes before this
				// current newline.
				//
				// as a result, we compute each line's indentation level only
				// after we fully tokenize and render the line itself.
				[_, :newline, _] -> {
					indents << indentLine(lastType)
					currs <- [curr]
					lines << ''
				}
				[_, :dot, _] -> add('.', 0)

				// opening delimiters
				[_, :leftParen, _] -> if {
					connectingToken?(lastType)
					lastType = :comma
					lastType = :ifKeyword
					lastType = :withKeyword -> add(' ' << render(token), 1)
					_ -> add(render(token), 1)
				}
				[_, :leftBracket, _]
				[_, :leftBrace, _] -> if {
					connectingToken?(lastType),
					lastType = :identifier
					lastType = :rightParen
					lastType = :rightBracket
					lastType = :rightBrace
					lastType = :ifKeyword
					lastType = :fnKeyword
					lastType = :withKeyword
					lastType = :comma -> add(' ' << render(token), 1)
					_ -> add(render(token), 1)
				}

				// if following a newline, these are exempt from the following rule
				[:newline, :rightParen, _]
				[:newline, :rightBracket, _]
				[:newline, :rightBrace, _] -> add(render(token), -1)

				// ensure { thing } and not {thing}
				[_, :rightParen, _]
				[_, :rightBracket, _] -> add(render(token), -1)
				[_, :rightBrace, _] -> if llastType {
					:leftParen, :rightParen,
					:leftBracket, :rightBracket,
					:leftBrace, :rightBrace -> add(render(token), -1)
					_ -> add(' ' << render(token), -1)
				}

				// no-ops, where we rely on automatic comma insertion
				[_, :comma, :newline]
				[_, :comma, :rightParen]
				[_, :comma, :rightBracket]
				[_, :comma, :rightBrace] -> ?

				// special cases for colon used in atoms
				[_, :colon, _] -> if  {
					lastType = :comma,
					connectingToken?(lastType) -> add(' ' << render(token), 0)
					_ -> add(':', 0)
				}

				// no-space tokens
				[_, :comma, _]
				[_, :ellipsis, _] -> add(render(token), 0)

				// parens and lists don't include inner space
				[:leftParen, _, _]
				[:leftBracket, _, _]
				// unary exprs or no-space binary exprs
				[:dot, _, _]
				[:exclam, _, _]
				[:minus, _, _] -> add(render(token), 0)

				// atom special case
				[:colon, :identifier, _] -> if llastType {
					// if token before colon cannot be end of an object key,
					// don't insert a space. This isn't a perfect rule but
					// seems to be a good-enough algorithm.
					:newline
					:comma
					:colon -> add(render(token), 0)
					_ -> add(' ' << render(token), 0)
				}

				[:leftBrace, _, _]
				_ -> add(' ' << render(token), 0)
			}
		}

		// indent last line
		indents << indentLine(last(tokens))

		// indentation collapsing:
		//
		// sometimes, multiple delimiter openers in a single line or a callback
		// being passed into a function will add multiple levels of indentation
		// in the above algorithm, but we only really want to indent one level
		// at a time visually, even if semantically there are multiple levels
		// of nesting present. We try to detect and "collapse" these
		// indentations into single levels of tab here.
		//
		// we do this by scanning lines and finding groups of lines that are
		// indented by more than 1 level at a time, and de-indenting them until
		// they're only indented one level.
		indents |> with each() fn(n, i) if i {
			0 -> ?
			_ -> if n < indents.(i - 1) {
				true -> {
					// backtrack to the line immediately following the first
					// line where indent > n
					fn sub(j) {
						if {
							indents.(j) > n -> sub(j - 1)
							_ -> j + 1
						}
					}
					target := sub(i - 1)

					// if the range from target to current line is indented by
					// more than 1, de-dent them accordingly
					if {
						indents.(target) - n > 1 -> {
							diff := indents.(target) - n
							range(target, i) |> with each() fn(j) {
								indents.(j) := indents.(j) - diff + 1
							}
						}
					}
				}
			}
		}

		indentedLines := lines |> with map() fn(line, i) if line {
			// we don't indent empty lines
			'' -> ''
			_ -> tabs(indents.(i)) << line
		}
		indentedLines |> join('\n')
	}

	{
		print: print
	}
}

// print takes an Oak source file and returns a pretty-printed string
fn print(text) {
	tokens := tokenize(text)
	Printer(tokens).print()
}

