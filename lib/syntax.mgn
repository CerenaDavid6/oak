// library for working with Mangolia token streams and syntax trees

{
	println: println
	slice: slice
	contains?: contains?
	map: map
	each: each
	filter: filter
	reduce: reduce
} := import('std')

{
	digit?: digit?
	word?: word?
	space?: space?
	trim: trim
} := import('str')

{
	format: format
} := import('fmt')

fn renderPos(pos) '[' + string(pos.0) + ':' + string(pos.1) + ']'

fn renderToken(token) if token.val {
	? -> format('{{ 0 }} {{ 1 }}', string(token.type), renderPos(token.pos))
	_ -> format('{{ 0 }}({{ 1 }}) {{ 2 }}', string(token.type), token.val, renderPos(token.pos))
}

// Tokenizer is a full-fidelity, lossless tokenizer for Magnolia. It produces a
// stream of valid Magnolia token types, plus any shebang, newlines, and
// comments it finds. To produce an AST, those non-standard non-AST tokens
// should be filtered out of the list first.
fn Tokenizer(source) {
	index := 0
	line := 1
	col := 0

	fn TokenAt(type, pos, val) {
		type: type
		val: val
		pos: pos
	}

	fn Token(type, val) TokenAt(type, [line, col], val)

	fn pos [line, col]
	fn eof? index = len(source)
	fn peek source.(index)
	fn peekAhead(n) if index + n > len(source) {
		true -> ' '
		_ -> source.(index + n)
	}
	fn next {
		char := source.(index)
		if index < len(source) {
			true -> index <- index + 1
		}
		if char {
			'\n' -> {
				line <- line + 1
				col <- 0
			}
			_ -> col <- col + 1
		}
		char
	}
	fn back {
		if index > 0 {
			true -> index <- index - 1
		}
		if source.(index) {
			// TODO: reset col correctly on backtrack
			'\n' -> line <- line - 1
			_ -> col <- col - 1
		}
	}

	fn readUntilChar(c) {
		fn sub(acc) if !eof?() & peek() != c {
			true -> sub(acc << next())
			_ -> acc
		}
		sub('')
	}
	fn readValidIdentifier {
		fn sub(acc) if eof?() {
			true -> acc
			_ -> {
				c := next()
				if word?(c) | c = '_' | c = '?' | c = '!' {
					true -> sub(acc << c)
					_ -> {
						back()
						acc
					}
				}
			}
		}
		sub('')
	}
	fn readValidNumeral {
		fn sub(acc) if eof?() {
			true -> acc
			_ -> {
				c := next()
				if digit?(c) | c = '.' {
					true -> sub(acc << c)
					_ -> {
						back()
						acc
					}
				}
			}
		}
		sub('')
	}
	fn nextToken if c := next() {
		',' -> Token(:comma)
		'.' -> if peek() = '.' & peekAhead(1) = '.' {
			true -> {
				pos := [line, col]
				next()
				next()
				TokenAt(:ellipsis, pos)
			}
			_ -> Token(:dot)
		}
		'(' -> Token(:leftParen)
		')' -> Token(:rightParen)
		'[' -> Token(:leftBracket)
		']' -> Token(:rightBracket)
		'{' -> Token(:leftBrace)
		'}' -> Token(:rightBrace)
		':' -> if peek() {
			'=' -> {
				pos := [line, col]
				next()
				TokenAt(:assign, pos)
			}
			_ -> Token(:colon)
		}
		'<' -> if peek() {
			'<' -> {
				pos := [line, col]
				next()
				TokenAt(:pushArrow, pos)
			}
			'-' -> {
				pos := [line, col]
				next()
				TokenAt(:nonlocalAssign, pos)
			}
			'=' -> {
				pos := [line, col]
				next()
				TokenAt(:leq, pos)
			}
			_ -> Token(:less)
		}
		'?' -> Token(:qmark)
		'!' -> if peek() {
			'=' -> {
				pos := [line, col]
				next()
				TokenAt(:neq, pos)
			}
			_ -> Token(:exclam)
		}
		'+' -> Token(:plus)
		'-' -> if peek() {
			'>' -> {
				pos := [line, col]
				next()
				TokenAt(:branchArrow, pos)
			}
			_ -> Token(:minus)
		}
		'*' -> Token(:times)
		'/' -> if peek() {
			'/' -> {
				// line comment
				pos := [line, col]
				next()
				commentString := readUntilChar('\n') |> trim()
				TokenAt(:comment, pos, commentString)
			}
			_ -> Token(:divide)
		}
		'%' -> Token(:modulus)
		'^' -> Token(:xor)
		'&' -> Token(:and)
		'|' -> if peek() {
			'>' -> {
				pos := [line, col]
				next()
				TokenAt(:pipeArrow, pos)
			}
			_ -> Token(:or)
		}
		'>' -> if peek() {
			'=' -> {
				pos := [line, col]
				next()
				TokenAt(:geq, pos)
			}
			_ -> Token(:greater)
		}
		'=' -> Token(:eq)
		'\'' -> {
			// TODO: support literal newlines, with extra tabs collapsed to newlines
			// TODO: support unicode escape sequences, like '\x10' = '\n' = char(10)
			fn sub(payload) if charInString := next() {
				? -> payload
				'\'' -> payload
				'\\' -> {
					if charInString := next() {
						'n' -> charInString := '\n'
						'r' -> charInString := '\r'
						'f' -> charInString := '\f'
						't' -> charInString := '\t'
					}
					sub(payload << charInString)
				}
				_ -> sub(payload << charInString)
			}
			pos := [line, col]
			stringContent := sub('')
			TokenAt(:stringLiteral, pos, stringContent)
		}
		_ -> {
			pos := [line, col]
			if digit?(c) {
				true -> {
					numberContent := c << readValidNumeral()
					TokenAt(:numberLiteral, pos, numberContent)
				}
				_ -> if payload := c << readValidIdentifier() {
					'_' -> TokenAt(:underscore, pos)
					'if' -> TokenAt(:ifKeyword, pos)
					'fn' -> TokenAt(:fnKeyword, pos)
					'with' -> TokenAt(:withKeyword, pos)
					'true' -> TokenAt(:trueLiteral, pos)
					'false' -> TokenAt(:falseLiteral, pos)
					_ -> TokenAt(:identifier, pos, payload)
				}
			}
		}
	}
	fn tokenize {
		tokens := []

		if peek() = '#' & peekAhead(1) = '!' {
			true -> {
				readUntilChar('\n')
				if !eof?() {
					true -> next()
				}
			}
		}

		// snip whitespace before
		fn eatSpace if space?(sp := peek()){
			true -> {
				if sp {
					'\n' -> tokens << Token(:newline)
				}
				next()
				eatSpace()
			}
		}
		eatSpace()

		lastTok := Token(:comma)
		fn sub {
			nextTok := nextToken()

			if !([:leftParen, :leftBracket, :leftBrace, :comma] |> contains?(lastTok.type)) &
				[:rightParen, :rightBracket, :rightBrace] |> contains?(nextTok.type) {
				true -> tokens << Token(:comma)
			}

			tokens << nextTok
			if nextTok.type {
				:comment -> nextTok := lastTok
			}

			// snip whitespace after
			fn eatSpaceAutoInsertComma if space?(peek()){
				true -> {
					if peek() {
						'\n' -> {
							if [
								:comma, :leftParen, :leftBracket, :leftBrace,
								:plus, :minus, :times, :divide, :modulus, :xor,
								:and, :or, :exclam, :greater, :less, :eq, :geq,
								:leq, :assign, :nonlocalAssign, :dot, :colon,
								:fnKeyword, :ifKeyword, :withKeyword,
								:pipeArrow, :branchArrow,
							] |> contains?(nextTok.type) {
								true -> ? // do nothing
								_ -> {
									nextTok <- Token(:comma)
									tokens << nextTok
								}
							}
							tokens << Token(:newline)
						}
					}
					next()
					eatSpaceAutoInsertComma()
				}
			}
			eatSpaceAutoInsertComma()

			if nextTok.type {
				:comment -> ?
				_ -> lastTok <- nextTok
			}

			if eof?() {
				false -> sub()
			}
		}

		// do not start tokenizing into empty file
		if eof?() {
			false -> sub()
		}

		if lastTok.type {
			:comma -> ?
			_ -> tokens << Token(:comma)
		}

		tokens
	}

	{
		tokenize: tokenize
	}
}

